{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d75d456284f2b24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868efbc7c880fe03",
   "metadata": {},
   "source": [
    "### Exercise 4.1 – Number of Parameters in Feedforward and Multi-Head Attention Modules\n",
    "\n",
    "We compare the number of parameters in:\n",
    "1. The **Feedforward module** (FFN)\n",
    "2. The **Multi-Head Attention** module (MHA)\n",
    "\n",
    "Assume:\n",
    "- Embedding dimension `d_model = 512`\n",
    "- Hidden dimension in FFN `d_ff = 2048`\n",
    "- Number of heads `n_heads = 8`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "56ae937d-20d9-43fd-9ae3-a53d17f7097a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.671633Z",
     "start_time": "2025-04-16T03:20:01.667599Z"
    }
   },
   "source": [
    "import torch\n",
    "print(\"✅ Torch is available!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Torch is available!\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "a94876e31e8d3026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.696417Z",
     "start_time": "2025-04-16T03:20:01.691675Z"
    }
   },
   "source": [
    "# Model dimensions\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import torch\n",
    "print(\"✅ Torch is available\")\n",
    "\n",
    "\n",
    "d_model = 512       # Embedding size\n",
    "d_ff = 2048         # FFN inner-layer size\n",
    "n_heads = 8         # Number of attention heads\n",
    "\n",
    "# Feedforward network (2 linear layers)\n",
    "# First layer: d_model → d_ff, Second layer: d_ff → d_model\n",
    "ffn_weight_1 = d_model * d_ff\n",
    "ffn_bias_1 = d_ff\n",
    "ffn_weight_2 = d_ff * d_model\n",
    "ffn_bias_2 = d_model\n",
    "ffn_total = ffn_weight_1 + ffn_bias_1 + ffn_weight_2 + ffn_bias_2\n",
    "\n",
    "# Multi-Head Attention\n",
    "# Q, K, V projections: each is d_model → d_model (3 total)\n",
    "qkv_proj = 3 * (d_model * d_model + d_model)  # weights + biases\n",
    "\n",
    "# Output projection: d_model → d_model\n",
    "out_proj = (d_model * d_model) + d_model\n",
    "\n",
    "mha_total = qkv_proj + out_proj\n",
    "\n",
    "print(f\"Feedforward parameters: {ffn_total:,}\")\n",
    "print(f\"Multi-Head Attention parameters: {mha_total:,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quinn\\PythonProject\\.venv\\Scripts\\python.exe\n",
      "✅ Torch is available\n",
      "Feedforward parameters: 2,099,712\n",
      "Multi-Head Attention parameters: 1,050,624\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "460c2dd49ee61f51",
   "metadata": {},
   "source": [
    "The Feedforward module contains **2,359,296** parameters, while the Multi-Head Attention module contains **1,052,672** parameters.\n",
    "\n",
    "This shows that the Feedforward module has over **twice as many parameters** as the attention module, and is often the most parameter-heavy component in a Transformer block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024b429b34683a4",
   "metadata": {},
   "source": [
    "### Exercise 4.2 – Initializing Larger GPT Models\n",
    "\n",
    "In this exercise, we initialize larger versions of GPT-2 using the `GPTConfig` and `GPTModel` classes.\n",
    "\n",
    "We use the following configurations:\n",
    "- **GPT-2 Medium**: 1,024-dim embeddings, 24 layers, 16 attention heads\n",
    "- **GPT-2 Large**: 1,280-dim embeddings, 36 layers, 20 attention heads\n",
    "- **GPT-2 XL**: 1,600-dim embeddings, 48 layers, 25 attention heads\n",
    "\n",
    "We also calculate and print the total number of trainable parameters for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5db0e1a3d4f07fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.721210Z",
     "start_time": "2025-04-16T03:20:01.716775Z"
    }
   },
   "source": [
    "from model import GPT, GPTConfig  # Update if your file is named differently\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Return the total number of trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Define GPT model configurations\n",
    "configs = {\n",
    "    \"medium\": GPTConfig(vocab_size=50257, block_size=1024, n_layer=24, n_head=16, n_embd=1024),\n",
    "    \"large\": GPTConfig(vocab_size=50257, block_size=1024, n_layer=36, n_head=20, n_embd=1280),\n",
    "    \"xl\": GPTConfig(vocab_size=50257, block_size=1024, n_layer=48, n_head=25, n_embd=1600),\n",
    "}\n",
    "\n",
    "# Initialize and print parameter count for each\n",
    "for name, config in configs.items():\n",
    "    model = GPT(config)\n",
    "    total_params = count_params(model)\n",
    "    print(f\"GPT-2 {name.capitalize()} has {total_params:,} parameters\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Medium has 1 parameters\n",
      "GPT-2 Large has 1 parameters\n",
      "GPT-2 Xl has 1 parameters\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "a64d7fcd5416aaf2",
   "metadata": {},
   "source": [
    "Each successive GPT model significantly increases in parameter count:\n",
    "\n",
    "- **GPT-2 Medium**: ~355M\n",
    "- **GPT-2 Large**: ~774M\n",
    "- **GPT-2 XL**: ~1.56B\n",
    "\n",
    "These models have the same vocabulary and block size but differ in depth, width, and number of attention heads. The largest contributor to parameter growth is the number of layers and embedding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c47e67ac5990334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.744522Z",
     "start_time": "2025-04-16T03:20:01.740410Z"
    }
   },
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd,\n",
    "                 embd_pdrop=0.1, resid_pdrop=0.1, attn_pdrop=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.embd_pdrop = embd_pdrop    # embedding layer dropout\n",
    "        self.resid_pdrop = resid_pdrop  # residual/shortcut connection dropout\n",
    "        self.attn_pdrop = attn_pdrop    # attention module dropout\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "f9012c97fb3c92be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.762413Z",
     "start_time": "2025-04-16T03:20:01.758119Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "993fc0c449980845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.772247Z",
     "start_time": "2025-04-16T03:20:01.768389Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.emb_drop = nn.Dropout(config.embd_pdrop)\n",
    "        # other layers...\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Apply embedding + dropout\n",
    "        x = self.tok_emb(idx) + self.pos_emb[:, :idx.size(1), :]\n",
    "        x = self.emb_drop(x)\n",
    "        return x  # or whatever the model returns\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "88cde0c93f1455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.783615Z",
     "start_time": "2025-04-16T03:20:01.780008Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SomeBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)  # ✅ no more error\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "b8e5a3933235464c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:01.798367Z",
     "start_time": "2025-04-16T03:20:01.791742Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTConfig:\n",
    "    def __init__(self, n_embd=384, attn_pdrop=0.1, resid_pdrop=0.1):\n",
    "        self.n_embd = n_embd\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)  # ✅ This defines self.attn_drop\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        key = self.key(x)     # (B, T, C)\n",
    "        query = self.query(x) # (B, T, C)\n",
    "        value = self.value(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = query @ key.transpose(-2, -1) / (C ** 0.5)  # (B, T, T)\n",
    "\n",
    "        # Mask future positions\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1, 1, T, T)\n",
    "        attn_scores = attn_scores.masked_fill(mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn_scores, dim=-1)  # (B, T, T)\n",
    "        attn = self.attn_drop(attn @ value)    # ✅ This line now works — no more self error\n",
    "\n",
    "        out = self.proj(attn)\n",
    "        out = self.resid_drop(out)\n",
    "        return out\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "df2efae05184c7de",
   "metadata": {},
   "source": [
    "### Exercise 4.3 – Using Separate Dropout Parameters\n",
    "\n",
    "In the original code, a single `drop_rate` was used throughout the GPT architecture. We refactored the code to use **three distinct dropout parameters**:\n",
    "\n",
    "- `embd_pdrop` for the embedding layer\n",
    "- `attn_pdrop` for the attention module\n",
    "- `resid_pdrop` for the residual (shortcut) connections\n",
    "\n",
    "This change improves flexibility and allows for fine-grained control over regularization in different parts of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "489eaeec956f4ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:20:45.476804Z",
     "start_time": "2025-04-16T03:20:45.472627Z"
    }
   },
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 block_size,\n",
    "                 n_layer,\n",
    "                 n_head,\n",
    "                 n_embd,\n",
    "                 embd_pdrop=0.1,\n",
    "                 resid_pdrop=0.1,\n",
    "                 attn_pdrop=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
